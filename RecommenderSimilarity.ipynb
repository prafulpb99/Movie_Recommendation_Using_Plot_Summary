{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommender System using Plot Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries\n",
    "\n",
    "NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices.\n",
    "Pandas is a library written for the Python programming language for data manipulation and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is imported as the data set is called movies1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies loaded: 5000 \n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(5)\n",
    "\n",
    "# Read in IMDb and Wikipedia movie data (both in same file)\n",
    "movies_df = pd.read_csv('movies1.csv')\n",
    "\n",
    "print(\"Number of movies loaded: %s \" % (len(movies_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 5000 rows of movie data are loaded and we can see a sample of 5 data rows below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Title</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6050</td>\n",
       "      <td>western</td>\n",
       "      <td>The Bounty Hunter</td>\n",
       "      <td>A prologue explains the role of the bounty hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6051</td>\n",
       "      <td>comedy</td>\n",
       "      <td>The Bowery Boys Meet the Monsters</td>\n",
       "      <td>The front window of Louie's Sweet Shop is a fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6052</td>\n",
       "      <td>war</td>\n",
       "      <td>The Bridges at Toko-Ri</td>\n",
       "      <td>U.S. Navy Lieutenant Harry Brubaker (William H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6053</td>\n",
       "      <td>musical</td>\n",
       "      <td>Brigadoon</td>\n",
       "      <td>Americans Tommy Albright (Gene Kelly) and Jeff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6054</td>\n",
       "      <td>drama</td>\n",
       "      <td>Bright Road</td>\n",
       "      <td>Jane Richards (Dorothy Dandridge) is a new tea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Genre                              Title  \\\n",
       "0        6050  western                  The Bounty Hunter   \n",
       "1        6051   comedy  The Bowery Boys Meet the Monsters   \n",
       "2        6052      war             The Bridges at Toko-Ri   \n",
       "3        6053  musical                          Brigadoon   \n",
       "4        6054    drama                        Bright Road   \n",
       "\n",
       "                                                Plot  \n",
       "0  A prologue explains the role of the bounty hun...  \n",
       "1  The front window of Louie's Sweet Shop is a fr...  \n",
       "2  U.S. Navy Lieutenant Harry Brubaker (William H...  \n",
       "3  Americans Tommy Albright (Gene Kelly) and Jeff...  \n",
       "4  Jane Richards (Dorothy Dandridge) is a new tea...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested only with the genre and the plot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "Genre         0\n",
      "Title         0\n",
      "Plot          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "movies_df = movies_df.dropna() # contain only the genre and plot features \n",
    "print(movies_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is clean and all the EDA has been performed and data has been replaced logically and now have the genre, title and the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "<p>Tokenization is the process  by which we break down articles into individual sentences or words, as needed. Besides the tokenization method provided by NLTK, we might have to perform additional filtration to remove tokens which are entirely numeric values or punctuation.</p>\n",
    "\n",
    "An example for a sentence is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sheetal\n",
      "[nltk_data]     Sekhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'a', 'good', 'day', 'to', 'be', 'alive']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize a paragraph into sentences and store in sent_tokenized\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "sent_tokenized = [sent for sent in nltk.sent_tokenize(\"\"\"\n",
    "                        Today is a good day to be alive. Life is beautiful\n",
    "                        \"\"\")]\n",
    "\n",
    "# Word Tokenize first sentence from sent_tokenized, save as words_tokenized\n",
    "words_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\n",
    "\n",
    "# Remove tokens that do not contain any letters from words_tokenized\n",
    "import re\n",
    "\n",
    "filtered = [word for word in words_tokenized if re.search('[a-zA-Z]', word)]\n",
    "\n",
    "# Display filtered words to observe words after tokenization\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has 4 columns and 5000 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose the language as English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of producing morphological variants of a root/base word. \n",
    "\n",
    "So variants of a word are reduced to the root word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "<p>Stemming is the process by which we bring down a word from its different forms to the root word. This helps us establish meaning to different forms of the same words without having to deal with each form separately.\n",
    "<p>There are different algorithms available for stemming such as the Porter Stemmer, Snowball Stemmer, etc. We shall use the Snowball Stemmer.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stemming:  ['Today', 'is', 'a', 'good', 'day', 'to', 'be', 'alive']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Create an English language SnowballStemmer object\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Print filtered to observe words without stemming\n",
    "print(\"Without stemming: \", filtered)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stemming:    ['today', 'is', 'a', 'good', 'day', 'to', 'be', 'aliv']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in filtered]\n",
    "\n",
    "print(\"After stemming:   \", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Club together Tokenize & Stem\n",
    "Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. \n",
    "\n",
    "One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n",
    "\n",
    "The below function does both tokenisation and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to perform both stemming and tokenization\n",
    "def tokenize_and_stem(text):\n",
    "    \n",
    "    # Tokenize by sentence, then by word\n",
    "    tokens = [word for sentence in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sentence)]\n",
    "    \n",
    "    # Filter out raw tokens to remove noise\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "    \n",
    "    # Stem the filtered_tokens\n",
    "    stems = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    \n",
    "    return stems\n",
    "\n",
    "words_stemmed = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")\n",
    "print(words_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn.feature_extraction.text. TfidfVectorizer has the advantage of emphasizing the most important words for a given document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function does exactly that to emphasise on that and derive meaning from the plot summary and suggest similar movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create TfidfVectorizer\n",
    "<p>Computers do not <em>understand</em> text. These are machines only capable of understanding numbers and performing numerical computation. Hence, we must convert our textual plot summaries to numbers for the computer to be able to extract meaning from them. \n",
    "<p> TF-IDF recognizes words which are unique and important to any given document. ## Club together Tokenize & Stem.\n",
    "    The below function does exactly that to emphasise on that and derive meaning from the plot summary and suggest similar movies\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "# Instantiate TfidfVectorizer object with stopwords and tokenizer\n",
    "# parameters for efficient processing of text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                 ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fit transform TfidfVectorizer\n",
    "<p>Once we create a TF-IDF Vectorizer, we must fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the <code>fit_transform()</code> method of the <code>TfidfVectorizer</code> object. </p>\n",
    "<p>If we observe the <code>TfidfVectorizer</code> object we created, we come across a parameter stopwords. 'stopwords' are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sheetal Sekhar\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 59)\t0.11140479903856593\n",
      "  (0, 56)\t0.3597652642368178\n",
      "  (0, 2)\t0.11695767935214813\n",
      "  (0, 42)\t0.11771636672836318\n",
      "  (0, 58)\t0.1026865764737806\n",
      "  (0, 51)\t0.46435116925307324\n",
      "  (0, 40)\t0.09898251735088832\n",
      "  (0, 63)\t0.10544904385768836\n",
      "  (0, 16)\t0.22288390485344234\n",
      "  (0, 61)\t0.1139223722860266\n",
      "  (0, 0)\t0.1134156754947762\n",
      "  (0, 7)\t0.37472436926936553\n",
      "  (0, 35)\t0.17779173516759503\n",
      "  (0, 54)\t0.38594378054310163\n",
      "  (0, 11)\t0.2169085591111055\n",
      "  (0, 33)\t0.10551450336093159\n",
      "  (0, 41)\t0.10321372258245412\n",
      "  (0, 25)\t0.1146729243026315\n",
      "  (0, 31)\t0.11716719689686722\n",
      "  (0, 15)\t0.10680958460299651\n",
      "  (0, 24)\t0.11360991115073629\n",
      "  (0, 1)\t0.10327613137556808\n",
      "  (0, 47)\t0.11621107777682275\n",
      "  (0, 32)\t0.08787323072338439\n",
      "  (0, 49)\t0.11971294068357613\n",
      "  :\t:\n",
      "  (4998, 28)\t0.08964473742541876\n",
      "  (4998, 12)\t0.10371948332060231\n",
      "  (4998, 3)\t0.6369690720547054\n",
      "  (4998, 45)\t0.08683601271467449\n",
      "  (4998, 39)\t0.0814044719113148\n",
      "  (4998, 18)\t0.08922421450238449\n",
      "  (4998, 5)\t0.09756496568757209\n",
      "  (4998, 65)\t0.1018712301706784\n",
      "  (4998, 26)\t0.0912058096276028\n",
      "  (4998, 14)\t0.1039424267479858\n",
      "  (4998, 60)\t0.09711992788053977\n",
      "  (4998, 19)\t0.09724659621792613\n",
      "  (4999, 2)\t0.2084408868495332\n",
      "  (4999, 51)\t0.20689058227058302\n",
      "  (4999, 40)\t0.17640572054356113\n",
      "  (4999, 61)\t0.20303139086579294\n",
      "  (4999, 41)\t0.18394653509969194\n",
      "  (4999, 32)\t0.15660685338298377\n",
      "  (4999, 10)\t0.1751458250230319\n",
      "  (4999, 12)\t0.41673281338708734\n",
      "  (4999, 22)\t0.41378116454116604\n",
      "  (4999, 23)\t0.19867707664669654\n",
      "  (4999, 4)\t0.18816378965439817\n",
      "  (4999, 18)\t0.5377387652939534\n",
      "  (4999, 46)\t0.2084408868495332\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"Plot\"]])\n",
    "\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  KMeans and create clusters\n",
    "To determine how closely one movie is related to the other by the help of unsupervised learning, we can use clustering techniques. Clustering is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters.\n",
    "\n",
    "A good basis of clustering in our dataset could be the genre of the movies.\n",
    "\n",
    "K-means is an algorithm which helps us to implement clustering in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1597\n",
       "4    1416\n",
       "1    1113\n",
       "2     460\n",
       "0     414\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import k-means to perform clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans object with 5 clusters and save as km\n",
    "km = KMeans(n_clusters=5)\n",
    "\n",
    "# Fit the k-means object with tfidf_matrix\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# Create a column cluster to denote the generated cluster for each movie\n",
    "movies_df[\"cluster\"] = clusters\n",
    "\n",
    "# Display number of films per cluster (clusters from 0 to 4)\n",
    "movies_df['cluster'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cosine_similarity to calculate similarity of movie plots\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate the similarity distance\n",
    "similarity_distance = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(similarity_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picke file- for serializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy our ML model on a websiteto increase the processing speed its better to pickle the result and then deploy it.\n",
    "The below code does it. It helps to load the data in a serialized manner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('simDist.pkl','wb') as f: pickle.dump(similarity_distance, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Similar Movie\n",
    "\n",
    "We can even create a function to search for the movie most similar to another.\n",
    "\n",
    "Here we are using a numpy methodargsoft to find de second most similar movie in the matrix. This is because the minimum distance of a movie is with itself. This can be seen in similarity_distance in the main diagonal(all the values are zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(title):\n",
    "  index = movies_df[movies_df['Title'] == title].index[0]\n",
    "  vector = similarity_distance[index, :]\n",
    "  most_similar = movies_df.iloc[np.argsort(vector)[1], 2]\n",
    "  return most_similar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Count Five and Die'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar(\"The Bounty Hunter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On Golden Pond'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar(\"The Exorcist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Training a machine learning model for basics tasks in NLP is simple. \n",
    "First, data is tokenized and filtered so that it can be represented with units called tokens. We can also represent the words to their root form, so the vocabulary can also be reduced and then we vectorize our dataset using an algorithm that depends on the problem we are trying to solve. \n",
    "\n",
    "Second, Here we used K-Means (Unsupervised Learning ) to group similar movies.\n",
    "\n",
    "Finally we had a function which took input as a movie and based on the above algorithms/score(similarity distance) , it gave us the most similar movie.\n",
    "\n",
    "This has also been deployed as a Web Application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "\n",
    "1. Towards Data Science\n",
    "2. Medium\n",
    "3. GitHub Repositories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
